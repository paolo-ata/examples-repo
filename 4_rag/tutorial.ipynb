{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Tutorial: Bauhaftpflicht-Fälle durchsuchen\n",
    "\n",
    "Dieses Notebook erklärt Schritt für Schritt, wie unser **Retrieval-Augmented Generation (RAG)**-System funktioniert.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Was ist RAG?\n",
    "\n",
    "RAG steht für **Retrieval-Augmented Generation** — ein Muster, bei dem ein LLM (Large Language Model) mit externem Wissen ergänzt wird.\n",
    "\n",
    "```\n",
    "Frage des Benutzers\n",
    "        │\n",
    "        ▼\n",
    "┌───────────────┐\n",
    "│   Retriever   │  ← Sucht relevante Dokumente in der Vektor-Datenbank\n",
    "└───────┬───────┘\n",
    "        │ Top-K Chunks\n",
    "        ▼\n",
    "┌───────────────┐\n",
    "│     LLM       │  ← Generiert Antwort basierend auf Frage + Kontext\n",
    "└───────┬───────┘\n",
    "        │\n",
    "        ▼\n",
    "   Antwort mit Quellen\n",
    "```\n",
    "\n",
    "**Ohne RAG** kennt das LLM nur sein Trainings-Wissen — es weiss nichts über unsere spezifischen Versicherungsfälle.\n",
    "\n",
    "**Mit RAG** füttern wir das LLM mit den relevantesten Dokumenten aus unserer Datenbank, bevor es antwortet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Warum RAG?\n",
    "\n",
    "LLMs haben drei fundamentale Probleme, die RAG löst:\n",
    "\n",
    "| Problem | Ohne RAG | Mit RAG |\n",
    "|---------|----------|--------|\n",
    "| **Halluzinationen** | LLM erfindet Fakten | Antwort basiert auf echten Dokumenten |\n",
    "| **Veraltetes Wissen** | Trainings-Cutoff | Zugriff auf aktuelle Daten |\n",
    "| **Kein Domänenwissen** | Kennt unsere Fälle nicht | Durchsucht unsere Falldatenbank |\n",
    "\n",
    "### Unser Anwendungsfall\n",
    "\n",
    "Ein Senior Underwriter für Bauhaftpflicht möchte wissen:\n",
    "- *\"Gab es ähnliche Fälle mit undichten Duschen?\"*\n",
    "- *\"Wie wurde in vergleichbaren Fällen die Haftung aufgeteilt?\"*\n",
    "- *\"Welche Beträge wurden bei Wasserschäden gezahlt?\"*\n",
    "\n",
    "Das LLM allein kann diese Fragen nicht beantworten — es braucht unsere Falldaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Die Komponenten\n",
    "\n",
    "### 3.1 Chunking\n",
    "\n",
    "Dokumente sind oft zu lang, um sie komplett dem LLM zu geben. Deshalb teilen wir sie in **Chunks** (Abschnitte).\n",
    "\n",
    "Wir verwenden den `RecursiveCharacterTextSplitter` von LangChain:\n",
    "- **chunk_size=1000**: Jeder Chunk ist ca. 1000 Zeichen lang\n",
    "- **chunk_overlap=200**: 200 Zeichen Überlappung, damit kein Kontext verloren geht\n",
    "- **Rekursiv**: Versucht zuerst an Absätzen zu trennen, dann an Sätzen, dann an Wörtern\n",
    "\n",
    "### 3.2 Embeddings\n",
    "\n",
    "Jeder Chunk wird durch ein **Embedding-Modell** in einen Zahlenvektor umgewandelt. Ähnliche Texte haben ähnliche Vektoren.\n",
    "\n",
    "Wir nutzen `text-embedding-3-small` von OpenAI.\n",
    "\n",
    "### 3.3 Vector Database (ChromaDB)\n",
    "\n",
    "Die Vektoren werden in **ChromaDB** gespeichert. Bei einer Suche wird die Frage ebenfalls in einen Vektor umgewandelt und die ähnlichsten Chunks werden zurückgegeben.\n",
    "\n",
    "### 3.4 LLM (gpt-4o-mini)\n",
    "\n",
    "Das LLM erhält die Frage zusammen mit den gefundenen Chunks und generiert eine strukturierte Antwort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Design-Entscheidung: Metadaten NICHT in den Chunk\n",
    "\n",
    "Eine wichtige Architektur-Entscheidung in diesem Projekt:\n",
    "\n",
    "### Schlechter Ansatz (3_rag)\n",
    "\n",
    "```\n",
    "page_content = \"\"\"\n",
    "[Fall W1 | Wasser/Abdichtung – Duschen | CHF 95'000]\n",
    "[Dokument: schadenanmeldung | 2024-02-20]\n",
    "---\n",
    "Wir erklären hiermit den Schadensfall...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Problem**: Der Header verfälscht die Embedding-Vektoren. Wenn jemand nach \"Wasseraustritt\" sucht, beeinflusst der Header-Text das Suchergebnis.\n",
    "\n",
    "### Guter Ansatz (unser Projekt)\n",
    "\n",
    "```python\n",
    "page_content = \"Wir erklären hiermit den Schadensfall...\"\n",
    "metadata = {\n",
    "    \"case_id\": \"W1\",\n",
    "    \"cluster\": \"Wasser/Abdichtung – Duschen\",\n",
    "    \"schaden_chf\": 95000,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Vorteil**: \n",
    "- Der Vektor repräsentiert nur den **echten Inhalt** des Dokuments\n",
    "- Metadaten stehen im `metadata`-Dict und können zum **Filtern** verwendet werden\n",
    "- Erst beim Prompt-Bau fügen wir den Metadaten-Header hinzu, damit das LLM die Quelle kennt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Code-Walkthrough\n",
    "\n",
    "### `config.py` — Zentrale Konfiguration\n",
    "\n",
    "Alle Pfade, Modellnamen und Parameter an einem Ort:\n",
    "\n",
    "```python\n",
    "DATA_PATH = BASE_DIR / \"..\" / \"2_syntetic_data\" / \"output\"  # Quelldaten\n",
    "CHROMA_PATH = BASE_DIR / \"chroma_db\"                        # Vektor-DB\n",
    "CHUNK_SIZE = 1000                                            # Chunk-Grösse\n",
    "CHUNK_OVERLAP = 200                                          # Überlappung\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"                   # Embedding-Modell\n",
    "LLM_MODEL = \"gpt-4o-mini\"                                   # LLM\n",
    "```\n",
    "\n",
    "### `indexer.py` — Dokumente laden und indexieren\n",
    "\n",
    "1. Liest alle Fall-Ordner (`W1`, `W2`, `F1`, ...)\n",
    "2. Lädt `case_bible.json` (Metadaten) und alle Dokument-JSONs\n",
    "3. Erstellt LangChain `Document`-Objekte mit reinem Text + Metadaten\n",
    "4. Chunked mit `RecursiveCharacterTextSplitter`\n",
    "5. Speichert in ChromaDB\n",
    "\n",
    "### `retriever.py` — Suche\n",
    "\n",
    "Eine generische `search()`-Funktion mit optionalem `filter_dict`:\n",
    "\n",
    "```python\n",
    "# Ohne Filter — alle Fälle durchsuchen\n",
    "search(\"Wasseraustritt\", k=5)\n",
    "\n",
    "# Mit Filter — nur Fall W1\n",
    "search(\"Wasseraustritt\", k=5, filter_dict={\"case_id\": \"W1\"})\n",
    "```\n",
    "\n",
    "### `rag_chain.py` — RAG-Logik\n",
    "\n",
    "1. Ruft `search()` auf\n",
    "2. Baut Prompt mit Metadaten-Header (zur Laufzeit, nicht im Vektor!)\n",
    "3. Sendet an LLM mit System-Prompt\n",
    "4. Dedupliziert Quellen\n",
    "\n",
    "### `app.py` — Streamlit Web-UI\n",
    "\n",
    "Bietet eine Web-Oberfläche mit:\n",
    "- Eingabefeld für Fragen\n",
    "- Sidebar-Filter für Fall-ID und Cluster\n",
    "- Anzeige der Antwort und Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interaktive Demos\n",
    "\n",
    "Jetzt wird's praktisch! Die folgenden Zellen kannst du direkt ausführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Indexierung durchführen\n",
    "\n",
    "Zuerst müssen wir die Dokumente in die Vektor-Datenbank laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexer import index_all_cases\n",
    "\n",
    "count = index_all_cases()\n",
    "print(f\"\\n==> {count} Chunks erfolgreich indexiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Suche ausführen und Chunks inspizieren\n",
    "\n",
    "Schauen wir uns an, was die Vektor-Suche zurückgibt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retriever import search, get_collection_stats\n",
    "\n",
    "# Statistiken anzeigen\n",
    "stats = get_collection_stats()\n",
    "print(f\"Collection: {stats}\\n\")\n",
    "\n",
    "# Suche durchführen\n",
    "results = search(\"Wasseraustritt im Duschbereich\", k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"=== Ergebnis {i + 1} (Score: {score:.3f}) ===\")\n",
    "    print(f\"Fall:    {doc.metadata.get('case_id')}\")\n",
    "    print(f\"Cluster: {doc.metadata.get('cluster')}\")\n",
    "    print(f\"Typ:     {doc.metadata.get('doc_typ')}\")\n",
    "    print(f\"Datum:   {doc.metadata.get('doc_datum')}\")\n",
    "    print(f\"CHF:     {doc.metadata.get('schaden_chf')}\")\n",
    "    print(f\"Inhalt:  {doc.page_content[:300]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Gefilterte Suche\n",
    "\n",
    "Wir können die Suche auf bestimmte Fälle oder Cluster einschränken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nur in Fall W1 suchen\n",
    "print(\"=== Suche nur in Fall W1 ===\")\n",
    "results_w1 = search(\"Abdichtung\", k=3, filter_dict={\"case_id\": \"W1\"})\n",
    "for doc, score in results_w1:\n",
    "    print(f\"  [{score:.3f}] {doc.metadata['doc_typ']} — {doc.page_content[:100]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Nur Fälle mit bestimmtem Cluster\n",
    "print(\"=== Suche im Cluster 'Wasser/Abdichtung – Duschen' ===\")\n",
    "results_cluster = search(\n",
    "    \"Haftung\",\n",
    "    k=3,\n",
    "    filter_dict={\"cluster\": \"Wasser/Abdichtung – Duschen\"},\n",
    ")\n",
    "for doc, score in results_cluster:\n",
    "    print(f\"  [{score:.3f}] Fall {doc.metadata['case_id']}: {doc.metadata['doc_typ']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Vollständige RAG-Anfrage mit Quellenangabe\n",
    "\n",
    "Jetzt das volle System: Suche + LLM-Antwort + Quellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_chain import ask\n",
    "\n",
    "result = ask(\n",
    "    \"Gibt es Fälle mit Wasserabdichtungsproblemen? Wie wurden sie gelöst?\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANTWORT:\")\n",
    "print(\"=\" * 60)\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\nQUELLEN:\")\n",
    "for s in result[\"sources\"]:\n",
    "    print(f\"  - Fall {s['case_id']}: {s['doc_typ']} ({s['doc_datum']}) — {s['cluster']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noch ein Beispiel: Mit Filter auf einen bestimmten Fall\n",
    "result = ask(\n",
    "    \"Was sind die wichtigsten Fakten zu diesem Fall?\",\n",
    "    filter_dict={\"case_id\": \"W3\"},\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANTWORT:\")\n",
    "print(\"=\" * 60)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Skalierung: Was passiert bei 400'000+ Dokumenten?\n",
    "\n",
    "Unser bisheriges System funktioniert gut für 20 Fälle. Aber was, wenn wir es produktiv einsetzen?\n",
    "\n",
    "### Die realen Zahlen\n",
    "\n",
    "```\n",
    "5 Jahre × 5'500 Fälle/Jahr × 15 Dokumente/Fall = ~412'500 Dokumente\n",
    "                                                 = ~2–4 Millionen Chunks\n",
    "```\n",
    "\n",
    "Bei dieser Grössenordnung stossen wir auf **drei fundamentale Probleme**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Nicht jede Frage ist eine Ähnlichkeitssuche\n",
    "\n",
    "Unser aktuelles System macht immer das Gleiche: Frage → Vektor-Suche → Top 5 Chunks → LLM. Aber in der Praxis gibt es sehr unterschiedliche Fragetypen:\n",
    "\n",
    "| Fragetyp | Beispiel | Was eigentlich nötig wäre |\n",
    "|-----------|---------|---------------------------|\n",
    "| **Vergleichsfall** | \"Gibt es ähnliche Fälle mit undichten Duschen?\" | Klassische Vektor-Suche — unser System kann das |\n",
    "| **Mitarbeiter-Analyse** | \"Wie erledigt Herr Müller seine Wasserschaden-Fälle?\" | Filtern nach Sachbearbeiter, dann *viele* Dokumente lesen und aggregieren |\n",
    "| **Abteilungs-Übersicht** | \"Wie viele offene Fälle hat Team Ost im Cluster Wasser?\" | Strukturierte Abfrage auf Metadaten (SQL-artig), kein Embedding nötig |\n",
    "| **Spezifischer Fall** | \"Was ist der aktuelle Stand von Fall W3?\" | Exakter Filter auf case_id, dann *alle* Dokumente dieses Falls laden |\n",
    "| **Norm-Recherche** | \"In welchen Fällen wurde OR 371 angewendet?\" | Keyword-Suche nach exaktem Begriff, Vektor-Suche versagt hier oft |\n",
    "\n",
    "**Top-K=5 löst nur den ersten Typ.** Für die restlichen brauchen wir andere Strategien.\n",
    "\n",
    "### Problem 2: Nadel im Heuhaufen\n",
    "\n",
    "Bei 3 Millionen Chunks liefert eine Vektor-Suche mit k=5 die **0.00017%** ähnlichsten Chunks. Die Wahrscheinlichkeit, relevante aber anders formulierte Treffer zu verpassen, ist sehr hoch. Besonders bei mehrsprachigen Dokumenten (DE/FR/IT) kann ein Treffer semantisch identisch, aber sprachlich völlig anders formuliert sein.\n",
    "\n",
    "### Problem 3: LLM-Kontextfenster ist begrenzt\n",
    "\n",
    "Selbst wenn du k=50 setzt, passen ca. 50'000 Zeichen in den Prompt. Bei analytischen Fragen (\"Übersicht über alle Wasserschaden-Fälle der letzten 3 Jahre\") bräuchtest du Hunderte von Dokumenten — das sprengt jedes Kontextfenster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lösung 1: ParentDocumentRetriever\n",
    "\n",
    "### Das Grundproblem\n",
    "\n",
    "Beim Chunking stehen wir vor einem Dilemma:\n",
    "- **Kleine Chunks** (200 Zeichen) → präzisere Suche, aber zu wenig Kontext fürs LLM\n",
    "- **Grosse Chunks** (2000 Zeichen) → genug Kontext fürs LLM, aber unpräzise Suche\n",
    "\n",
    "### Die Lösung: Zwei Ebenen\n",
    "\n",
    "Der ParentDocumentRetriever löst das elegant mit zwei Chunk-Grössen:\n",
    "\n",
    "```\n",
    "Originaldokument (z.B. 5'000 Zeichen)\n",
    "    │\n",
    "    ├── Parent-Chunk 1 (2000 Zeichen)  ← Wird ans LLM gegeben\n",
    "    │       ├── Child-Chunk 1a (200 Zeichen)  ← Wird embedded + gesucht\n",
    "    │       ├── Child-Chunk 1b (200 Zeichen)  ← Wird embedded + gesucht\n",
    "    │       └── Child-Chunk 1c (200 Zeichen)  ← Wird embedded + gesucht\n",
    "    │\n",
    "    └── Parent-Chunk 2 (2000 Zeichen)  ← Wird ans LLM gegeben\n",
    "            ├── Child-Chunk 2a (200 Zeichen)  ← Wird embedded + gesucht\n",
    "            └── Child-Chunk 2b (200 Zeichen)  ← Wird embedded + gesucht\n",
    "```\n",
    "\n",
    "**Ablauf:**\n",
    "1. Benutzer stellt Frage\n",
    "2. Vektor-Suche findet den besten **Child-Chunk** (klein, präzise)\n",
    "3. System holt den zugehörigen **Parent-Chunk** (gross, kontextreich)\n",
    "4. Parent-Chunk wird ans LLM gegeben\n",
    "\n",
    "### Wann sinnvoll?\n",
    "\n",
    "- Bei langen Dokumenten (Expertisen, Klageantworten, Urteile)\n",
    "- Wenn die Suchpräzision bei vielen Chunks leidet\n",
    "- Gut kombinierbar mit allen anderen Lösungen unten\n",
    "\n",
    "### Limitierung\n",
    "\n",
    "Löst **nicht** das Problem der verschiedenen Fragetypen — es verbessert nur die Qualität der Vektor-Suche selbst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Lösung 2: Query Routing\n",
    "\n",
    "### Idee\n",
    "\n",
    "Bevor wir überhaupt suchen, lassen wir ein LLM die Frage **klassifizieren** und wählen dann die passende Strategie:\n",
    "\n",
    "```\n",
    "Benutzerfrage\n",
    "      │\n",
    "      ▼\n",
    "┌─────────────────┐\n",
    "│  LLM-Classifier │  \"Was für eine Frage ist das?\"\n",
    "└────────┬────────┘\n",
    "         │\n",
    "    ┌────┼──────────┬──────────────┐\n",
    "    ▼    ▼          ▼              ▼\n",
    " Vektor  Metadata   Keyword    Alle Docs\n",
    " Suche   Filter     Suche      eines Falls\n",
    "```\n",
    "\n",
    "### Konkret\n",
    "\n",
    "Ein kleiner Prompt klassifiziert die Frage:\n",
    "\n",
    "```python\n",
    "ROUTING_PROMPT = \"\"\"Klassifiziere die Frage in eine Kategorie:\n",
    "- SIMILARITY: Suche nach ähnlichen Fällen\n",
    "- CASE_LOOKUP: Frage zu einem spezifischen Fall (enthält Fall-ID)\n",
    "- AGGREGATION: Übersicht, Statistik, Zählung\n",
    "- KEYWORD: Suche nach exaktem Begriff (Norm, Gesetz, Name)\n",
    "\n",
    "Frage: {question}\n",
    "Kategorie:\"\"\"\n",
    "```\n",
    "\n",
    "Je nach Ergebnis wird eine andere Suchstrategie verwendet:\n",
    "- **SIMILARITY** → Vektor-Suche mit k=10 + Reranking\n",
    "- **CASE_LOOKUP** → Metadata-Filter `{\"case_id\": \"W3\"}`, alle Dokumente laden\n",
    "- **AGGREGATION** → SQL-Query auf Metadaten-Tabelle\n",
    "- **KEYWORD** → BM25 Volltextsuche statt Embedding\n",
    "\n",
    "### Vorteil\n",
    "\n",
    "Einfach zu implementieren, ein LLM-Call für die Klassifikation, dann deterministische Logik.\n",
    "\n",
    "### Limitierung\n",
    "\n",
    "Funktioniert nur, wenn die Fragetypen klar trennbar sind. Bei komplexen Fragen, die *mehrere Strategien* brauchen (\"Wie hat Herr Müller ähnliche Wasserschäden gelöst und wie viele waren es?\"), stösst Routing an seine Grenzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Lösung 3: Agentic RAG\n",
    "\n",
    "Das ist die mächtigste — aber auch komplexeste — Lösung. Hier wird das Konzept konkret erklärt.\n",
    "\n",
    "### Was ist ein \"Agent\"?\n",
    "\n",
    "Ein Agent ist ein LLM, das **nicht einfach antwortet**, sondern **selbst entscheidet, was es als Nächstes tun muss**. Es hat Zugang zu **Tools** (Werkzeugen) und ruft diese in einer Schleife auf, bis es genug Information hat, um zu antworten.\n",
    "\n",
    "```\n",
    "Benutzerfrage\n",
    "      │\n",
    "      ▼\n",
    "┌──────────────────────────────────────────────────────┐\n",
    "│  AGENT (LLM in einer Schleife)                       │\n",
    "│                                                      │\n",
    "│  Schritt 1: \"Ich muss zuerst herausfinden, welche   │\n",
    "│              Fälle Herr Müller bearbeitet hat.\"       │\n",
    "│              → Ruft Tool: metadata_query auf          │\n",
    "│                                                      │\n",
    "│  Schritt 2: \"OK, Müller hat 47 Fälle. Davon sind    │\n",
    "│              12 im Cluster Wasser. Ich schaue mir     │\n",
    "│              die Abschluss-Dokumente an.\"             │\n",
    "│              → Ruft Tool: vector_search auf           │\n",
    "│                                                      │\n",
    "│  Schritt 3: \"Die Chunks zeigen ein Muster. Ich       │\n",
    "│              brauche noch die Vergleichsbeträge.\"     │\n",
    "│              → Ruft Tool: metadata_query auf          │\n",
    "│                                                      │\n",
    "│  Schritt 4: \"Jetzt habe ich genug Info.\"             │\n",
    "│              → Generiert finale Antwort               │\n",
    "└──────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Die Tools des Agenten\n",
    "\n",
    "Das Entscheidende bei Agentic RAG sind die **Tools**, die dem Agenten zur Verfügung stehen. Der Agent \"sieht\" für jedes Tool eine Beschreibung und entscheidet selbst, wann er welches Tool einsetzt.\n",
    "\n",
    "#### Tool 1: `vector_search`\n",
    "Semantische Ähnlichkeitssuche — das, was unser aktuelles System macht.\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def vector_search(query: str, k: int = 10, filter_dict: dict = None) -> str:\n",
    "    \"\"\"Durchsucht die Falldatenbank nach semantisch ähnlichen Dokumenten.\n",
    "    \n",
    "    Nutze dieses Tool, wenn du nach inhaltlich ähnlichen Fällen, \n",
    "    Argumentationen oder Sachverhalten suchst.\n",
    "    \n",
    "    Args:\n",
    "        query: Die Suchanfrage in natürlicher Sprache\n",
    "        k: Anzahl Ergebnisse (Standard: 10)\n",
    "        filter_dict: Optionaler Filter, z.B. {\"case_id\": \"W1\"}\n",
    "    \"\"\"\n",
    "    results = retriever.search(query, k=k, filter_dict=filter_dict)\n",
    "    # Formatiert als lesbarer Text für den Agenten\n",
    "    return format_results(results)\n",
    "```\n",
    "\n",
    "#### Tool 2: `metadata_query`\n",
    "Strukturierte Abfrage auf den Metadaten — wie eine SQL-Query.\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def metadata_query(\n",
    "    filter_dict: dict, \n",
    "    return_fields: list[str] = None,\n",
    "    count_only: bool = False\n",
    ") -> str:\n",
    "    \"\"\"Durchsucht die Metadaten der Fälle strukturiert.\n",
    "    \n",
    "    Nutze dieses Tool für Zählungen, Übersichten und Filterungen,\n",
    "    wenn du NICHT nach Textinhalten suchst, sondern nach strukturierten\n",
    "    Informationen wie: Anzahl Fälle, Liste von Fall-IDs, Beträge, Status.\n",
    "    \n",
    "    Args:\n",
    "        filter_dict: Filter, z.B. {\"sachbearbeiter\": \"Müller\", \"status\": \"offen\"}\n",
    "        return_fields: Welche Felder zurückgeben, z.B. [\"case_id\", \"schaden_chf\"]\n",
    "        count_only: Wenn True, nur die Anzahl Treffer zurückgeben\n",
    "    \n",
    "    Beispiele:\n",
    "        - Wie viele offene Fälle? → filter={\"status\": \"offen\"}, count_only=True\n",
    "        - Alle Fälle von Müller? → filter={\"sachbearbeiter\": \"Müller\"}, \n",
    "                                    return_fields=[\"case_id\", \"cluster\", \"schaden_chf\"]\n",
    "    \"\"\"\n",
    "    # Abfrage auf Metadaten-Tabelle (z.B. PostgreSQL oder Pandas DataFrame)\n",
    "    return query_metadata_store(filter_dict, return_fields, count_only)\n",
    "```\n",
    "\n",
    "#### Tool 3: `get_case_documents`\n",
    "Lädt *alle* Dokumente eines bestimmten Falls.\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def get_case_documents(case_id: str, doc_typ: str = None) -> str:\n",
    "    \"\"\"Lädt alle Dokumente eines bestimmten Falls.\n",
    "    \n",
    "    Nutze dieses Tool, wenn du einen bestimmten Fall im Detail \n",
    "    analysieren musst — z.B. alle Dokumente von Fall W3 oder nur \n",
    "    das Urteil von Fall H2.\n",
    "    \n",
    "    Args:\n",
    "        case_id: Die Fall-ID, z.B. \"W3\"\n",
    "        doc_typ: Optional: Nur Dokumente dieses Typs, z.B. \"urteilsauszug\"\n",
    "    \"\"\"\n",
    "    docs = load_all_documents_for_case(case_id, doc_typ)\n",
    "    return format_documents(docs)\n",
    "```\n",
    "\n",
    "#### Tool 4: `summarize_text`\n",
    "Fasst lange Texte zusammen — wichtig, wenn der Agent zu viel Text hat.\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def summarize_text(text: str, focus: str = None) -> str:\n",
    "    \"\"\"Fasst einen langen Text zusammen.\n",
    "    \n",
    "    Nutze dieses Tool, wenn du zu viel Text hast, um ihn direkt\n",
    "    zu verarbeiten. Gib optional einen Fokus an, z.B. \n",
    "    \"Konzentriere dich auf die Haftungsfrage\".\n",
    "    \n",
    "    Args:\n",
    "        text: Der zu zusammenfassende Text\n",
    "        focus: Optionaler Fokus für die Zusammenfassung\n",
    "    \"\"\"\n",
    "    return llm_summarize(text, focus)\n",
    "```\n",
    "\n",
    "### Konkretes Beispiel: Komplexe Frage\n",
    "\n",
    "**Frage:** *\"Wie erledigt Herr Müller seine Wasserschaden-Fälle typischerweise? Gibt es ein Muster?\"*\n",
    "\n",
    "So würde der Agent vorgehen:\n",
    "\n",
    "```\n",
    "Agent denkt: \"Das ist eine analytische Frage über einen Mitarbeiter.\n",
    "              Ich muss zuerst seine Fälle finden, dann die Wasserschäden\n",
    "              filtern und dann die Abschluss-Dokumente analysieren.\"\n",
    "\n",
    "→ Tool-Aufruf 1: metadata_query(\n",
    "      filter_dict={\"sachbearbeiter\": \"Müller\", \"cluster\": {\"$contains\": \"Wasser\"}},\n",
    "      return_fields=[\"case_id\", \"status\", \"schaden_chf\"]\n",
    "  )\n",
    "← Ergebnis: \"12 Fälle gefunden: W1 (vergleich, CHF 95'000), W4 (vergleich, CHF 42'000), ...\"\n",
    "\n",
    "Agent denkt: \"OK, 12 Fälle, die meisten per Vergleich erledigt.\n",
    "              Ich schaue mir die Vergleichsangebote und Abschlüsse genauer an.\"\n",
    "\n",
    "→ Tool-Aufruf 2: vector_search(\n",
    "      query=\"Vergleichsverhandlung Einigung Abschluss\",\n",
    "      k=15,\n",
    "      filter_dict={\"sachbearbeiter\": \"Müller\", \"cluster\": {\"$contains\": \"Wasser\"}}\n",
    "  )\n",
    "← Ergebnis: 15 Chunks aus verschiedenen Abschluss-Dokumenten\n",
    "\n",
    "Agent denkt: \"Das ist viel Text. Ich fasse die Muster zusammen.\"\n",
    "\n",
    "→ Tool-Aufruf 3: summarize_text(\n",
    "      text=[die 15 Chunks],\n",
    "      focus=\"Muster in der Vorgehensweise bei Vergleichsverhandlungen\"\n",
    "  )\n",
    "← Ergebnis: \"Zusammenfassung: Müller holt typischerweise zuerst ein \n",
    "             Parteigutachten ein, dann eine gemeinsame Expertise, und \n",
    "             schliesst meist mit einem Vergleich bei 40-60% der Forderung ab.\"\n",
    "\n",
    "Agent denkt: \"Jetzt habe ich genug Information für eine fundierte Antwort.\"\n",
    "\n",
    "→ Generiert finale Antwort mit Quellen\n",
    "```\n",
    "\n",
    "### Der entscheidende Unterschied zu \"normalem\" RAG\n",
    "\n",
    "| | Normales RAG | Agentic RAG |\n",
    "|---|---|---|\n",
    "| **Ablauf** | Fix: Suche → Prompt → Antwort | Dynamisch: Agent entscheidet bei jeder Frage neu |\n",
    "| **Anzahl Suchen** | Immer genau 1 | So viele wie nötig (typisch 2–5) |\n",
    "| **Suchstrategie** | Immer Vektor-Suche | Wählt aus: Vektor, Metadata, Keyword, Volldokument |\n",
    "| **Umgang mit viel Text** | Alles in einen Prompt | Kann zwischendurch zusammenfassen |\n",
    "| **Kosten** | 1 LLM-Call | 3–10 LLM-Calls pro Frage |\n",
    "| **Latenz** | ~2 Sekunden | ~10–30 Sekunden |\n",
    "| **Komplexität** | Einfach | Deutlich komplexer zu bauen und debuggen |\n",
    "\n",
    "### Frameworks für Agentic RAG\n",
    "\n",
    "- **LangGraph** (von LangChain) — Graph-basierte Agent-Logik, gut für kontrollierte Abläufe\n",
    "- **LlamaIndex Agents** — Eingebaute RAG-Agenten mit Query Planning\n",
    "- **CrewAI** — Multi-Agenten-System (z.B. ein \"Recherche-Agent\" und ein \"Analyse-Agent\")\n",
    "- **Eigenbau mit OpenAI Function Calling** — Maximale Kontrolle, mehr Aufwand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Lösung 4: Hybrid Search + Reranking\n",
    "\n",
    "### Hybrid Search\n",
    "\n",
    "Kombination aus zwei Suchmethoden:\n",
    "\n",
    "```\n",
    "Frage: \"Wurde OR 371 in Fällen mit Abdichtungsmängeln angewendet?\"\n",
    "      │\n",
    "      ├── Vektor-Suche → Findet semantisch ähnliche Chunks\n",
    "      │                   (gut für \"Abdichtungsmängel\")\n",
    "      │\n",
    "      ├── Keyword-Suche (BM25) → Findet exakte Begriffe\n",
    "      │                           (gut für \"OR 371\")\n",
    "      │\n",
    "      ▼\n",
    "  Ergebnisse verschmelzen (Reciprocal Rank Fusion)\n",
    "      │\n",
    "      ▼\n",
    "  Kombinierte Top-K Ergebnisse\n",
    "```\n",
    "\n",
    "**Warum beides?** Vektor-Suche versteht Bedeutung (\"Wasseraustritt\" ≈ \"infiltration d'eau\"), aber versagt bei exakten Fachbegriffen. Keyword-Suche findet \"OR 371\" zuverlässig, versteht aber keine Synonyme.\n",
    "\n",
    "### Reranking\n",
    "\n",
    "Ein zweistufiger Prozess: zuerst breit suchen, dann präzise filtern.\n",
    "\n",
    "```\n",
    "Schritt 1: Vektor-Suche mit k=50  (schnell, aber ungenau)\n",
    "                │\n",
    "                ▼\n",
    "Schritt 2: Cross-Encoder bewertet alle 50 Ergebnisse neu\n",
    "           (langsam, aber viel präziser)\n",
    "                │\n",
    "                ▼\n",
    "Schritt 3: Nur die besten 5–10 ans LLM\n",
    "```\n",
    "\n",
    "Ein **Cross-Encoder** (z.B. `cross-encoder/ms-marco-MiniLM-L-6-v2`) bewertet Frage+Chunk als *Paar* — das ist deutlich genauer als die Vektor-Ähnlichkeit, aber zu langsam für Millionen von Chunks. Deshalb die Zweistufigkeit.\n",
    "\n",
    "### Wann sinnvoll?\n",
    "\n",
    "- Hybrid Search: Immer, wenn exakte Begriffe wichtig sind (Normen, Gesetze, Namen, Fall-IDs)\n",
    "- Reranking: Bei grossem Corpus, wo die Top-5 der Vektor-Suche oft nicht die besten Treffer sind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Lösung 5: Skalierbare Vector-Datenbank\n",
    "\n",
    "ChromaDB ist hervorragend zum Lernen und für Prototypen. Bei 2–4 Millionen Chunks braucht man aber eine produktionsreife Lösung:\n",
    "\n",
    "| Datenbank | Typ | Hybrid Search | Besonderheit |\n",
    "|-----------|-----|:---:|---|\n",
    "| **Qdrant** | Self-hosted oder Cloud | Ja | Sehr schnell, guter Filter-Support, Rust-basiert |\n",
    "| **Weaviate** | Self-hosted oder Cloud | Ja | GraphQL API, eingebaute Hybrid Search |\n",
    "| **Pinecone** | Nur Cloud (Managed) | Ja | Skaliert automatisch, kein Ops-Aufwand |\n",
    "| **pgvector** | PostgreSQL-Extension | Nein* | Ideal wenn Postgres schon vorhanden, einfache Integration |\n",
    "| **Milvus** | Self-hosted oder Cloud | Ja | Für sehr grosse Datenmengen (Milliarden Vektoren) |\n",
    "\n",
    "*pgvector kann mit `pg_trgm` oder Volltextsuche kombiniert werden.\n",
    "\n",
    "### Entscheidungshilfe\n",
    "\n",
    "- **Ihr habt schon PostgreSQL** → pgvector (geringster Aufwand)\n",
    "- **Maximale Kontrolle, self-hosted** → Qdrant (beste Performance/Preis)\n",
    "- **Kein Ops-Team, einfach skalieren** → Pinecone (Managed Service)\n",
    "- **Multi-Tenant, viele Teams** → Weaviate (gute Mandantentrennung)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Empfohlene Ausbaustufen\n",
    "\n",
    "Nicht alles auf einmal bauen! Ein pragmatischer Stufenplan:\n",
    "\n",
    "```\n",
    "Stufe 0 (jetzt)     Stufe 1              Stufe 2              Stufe 3\n",
    "─────────────────    ─────────────────    ─────────────────    ─────────────────\n",
    "Einfaches RAG        + Hybrid Search      + Query Routing      + Agentic RAG\n",
    "ChromaDB             + Reranking          + Metadata-Queries   + Multi-Tool Agent\n",
    "Top-K=5              + Qdrant/pgvector    + ParentDoc-Retr.    + Zusammenfassung\n",
    "                     + Grösseres K                             + Evaluation\n",
    "                                                                 (RAGAS)\n",
    "```\n",
    "\n",
    "| Stufe | Löst | Aufwand | Wann umsetzen? |\n",
    "|-------|------|---------|----------------|\n",
    "| **0** | Grundfunktion: ähnliche Fälle finden | Fertig (dieses Projekt) | Jetzt |\n",
    "| **1** | Bessere Trefferqualität bei vielen Daten | 1–2 Wochen | Sobald die echten Daten da sind |\n",
    "| **2** | Verschiedene Fragetypen (Mitarbeiter, Übersicht, spez. Fall) | 2–3 Wochen | Wenn Benutzer unterschiedliche Fragen stellen |\n",
    "| **3** | Komplexe, mehrstufige Analysen | 3–5 Wochen | Wenn die anderen Stufen nicht mehr reichen |\n",
    "\n",
    "Jede Stufe baut auf der vorherigen auf — nichts muss weggeworfen werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
