{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGer Scraper - Code-Erklärung für Anfänger\n",
    "\n",
    "In diesem Notebook erkläre ich dir Schritt für Schritt, wie der Web Scraper funktioniert, den wir für die Bundesgerichts-Regesten geschrieben haben.\n",
    "\n",
    "## Was ist Web Scraping?\n",
    "\n",
    "**Web Scraping** bedeutet, automatisiert Daten von Websites zu extrahieren. Statt manuell jede Seite zu besuchen und den Text zu kopieren, schreibt man ein Programm, das dies automatisch macht.\n",
    "\n",
    "**Unser Ziel:** Alle Regesten aus BGE 151 III automatisch in eine CSV-Datei exportieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Teil 1: HTML-Grundlagen\n",
    "\n",
    "Bevor wir den Code verstehen können, müssen wir verstehen, wie Websites aufgebaut sind.\n",
    "\n",
    "## Was ist HTML?\n",
    "\n",
    "**HTML** (HyperText Markup Language) ist die Sprache, in der Websites geschrieben sind. Es besteht aus **Tags**, die den Inhalt strukturieren.\n",
    "\n",
    "### Beispiel einer einfachen HTML-Struktur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dies ist KEIN Python-Code, sondern zeigt dir, wie HTML aussieht:\n",
    "\n",
    "beispiel_html = \"\"\"\n",
    "<html>\n",
    "\n",
    "  <head>\n",
    "    <title>Meine Webseite</title>\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <h1>Willkommen</h1>\n",
    "    <p>Dies ist ein Absatz.</p>\n",
    "    <div class=\"wichtig\">\n",
    "      <span>Text in einem Span</span>\n",
    "    </div>\n",
    "  </body>\n",
    "\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"So sieht HTML aus:\")\n",
    "print(beispiel_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die wichtigsten HTML-Begriffe:\n",
    "\n",
    "| Begriff | Erklärung | Beispiel |\n",
    "|---------|-----------|----------|\n",
    "| **Tag** | Ein HTML-Element, eingeschlossen in `< >` | `<p>`, `<div>`, `<span>` |\n",
    "| **Öffnender Tag** | Beginnt ein Element | `<div>` |\n",
    "| **Schliessender Tag** | Beendet ein Element (mit `/`) | `</div>` |\n",
    "| **Attribut** | Zusätzliche Information im Tag | `class=\"wichtig\"`, `id=\"regeste\"` |\n",
    "| **class** | Eine Kategorie/Gruppe (kann mehrfach vorkommen) | `<div class=\"big bold\">` |\n",
    "| **id** | Ein eindeutiger Bezeichner | `<div id=\"regeste\">` |\n",
    "\n",
    "### Häufige HTML-Tags:\n",
    "\n",
    "- `<div>` - Ein Block-Container (wie eine Box)\n",
    "- `<span>` - Ein Inline-Container (für Text innerhalb einer Zeile)\n",
    "- `<p>` - Ein Absatz (Paragraph)\n",
    "- `<a href=\"...\">` - Ein Link\n",
    "- `<h1>`, `<h2>`, etc. - Überschriften"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die HTML-Struktur der BGer-Website\n",
    "\n",
    "Um einen Scraper zu schreiben, muss man zuerst die HTML-Struktur der Website analysieren. \n",
    "\n",
    "**So habe ich das gemacht:**\n",
    "1. Website im Browser öffnen\n",
    "2. Rechtsklick → \"Untersuchen\" (oder F12 für Entwicklertools)\n",
    "3. Die HTML-Struktur anschauen und Muster erkennen\n",
    "\n",
    "### Was ich auf der BGer-Website gefunden habe:\n",
    "\n",
    "Jede Regeste ist in einem `<div>` mit `id=\"regeste\"` eingeschlossen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sieht die HTML-Struktur einer Regeste auf bger.ch aus:\n",
    "\n",
    "bger_html_beispiel = \"\"\"\n",
    "<div id=\"regeste\" lang=\"de\">\n",
    "  <div class=\"big bold\">Regeste</div>\n",
    "  <br>\n",
    "  <div class=\"paraatf\">\n",
    "    <span class=\"artref\">Art. 125 ZGB</span>; nachehelicher Unterhalt bei Scheidung.\n",
    "    <div class=\"paratf\">\n",
    "      Die Rechtsprechung, wonach die Pflicht zur Leistung nachehelichen \n",
    "      Unterhaltes im Grundsatz längstens bis zum Erreichen des ordentlichen \n",
    "      Pensionierungsalters dauert...\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "print(\"HTML-Struktur einer BGer-Regeste:\")\n",
    "print(bger_html_beispiel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse der Struktur:\n",
    "\n",
    "```\n",
    "<div id=\"regeste\">           ← Container für die gesamte Regeste\n",
    "  │\n",
    "  ├── <div class=\"big bold\">  ← Das Label (\"Regeste\" oder \"Regeste a\")\n",
    "  │\n",
    "  └── <div class=\"paraatf\">   ← Der Inhalt\n",
    "        │\n",
    "        ├── <span class=\"artref\">  ← Gesetzesartikel\n",
    "        │\n",
    "        └── <div class=\"paratf\">   ← Weiterer Text\n",
    "```\n",
    "\n",
    "**Wichtige Erkenntnis:** Bei Entscheiden mit mehreren Regesten gibt es mehrere `<div id=\"regeste\">` Blöcke!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bei mehreren Regesten sieht es so aus:\n",
    "\n",
    "mehrere_regesten_html = \"\"\"\n",
    "<div id=\"regeste\" lang=\"de\">\n",
    "  <div class=\"big bold\">Regeste a</div>\n",
    "  <div class=\"paraatf\">\n",
    "    <span class=\"artref\">Art. 51 Abs. 4 BGG</span>; Streitwert...\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div id=\"regeste\" lang=\"de\">\n",
    "  <div class=\"big bold\">Regeste b</div>\n",
    "  <div class=\"paraatf\">\n",
    "    <span class=\"artref\">Art. 84 SchKG</span>; Kognition...\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "print(\"HTML bei mehreren Regesten (z.B. BGE 151 III 45):\")\n",
    "print(mehrere_regesten_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Teil 2: Das Kerngerüst des Scrapers\n",
    "\n",
    "Jetzt schauen wir uns den eigentlichen Python-Code an.\n",
    "\n",
    "## Der Ablauf in 4 Schritten:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  1. INDEX-SEITE LADEN                                          │\n",
    "│     → Liste aller BGE 151 III Entscheide holen                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  2. LINKS EXTRAHIEREN                                          │\n",
    "│     → URLs zu allen Einzelentscheiden sammeln                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  3. JEDEN ENTSCHEID BESUCHEN                                   │\n",
    "│     → Urteilsnummer und Regeste extrahieren                    │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                              ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│  4. CSV SCHREIBEN                                              │\n",
    "│     → Alle Daten in eine Datei speichern                       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Imports - Was brauchen wir?\n",
    "\n",
    "Schauen wir uns zuerst die benötigten Bibliotheken an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diese Bibliotheken werden importiert:\n",
    "\n",
    "import csv                      # Zum Schreiben von CSV-Dateien\n",
    "import re                       # Regular Expressions (Textmuster suchen)\n",
    "import time                     # Zum Warten zwischen Anfragen\n",
    "from urllib.parse import urljoin  # URLs zusammenbauen\n",
    "\n",
    "import requests                 # HTTP-Anfragen an Websites senden\n",
    "from bs4 import BeautifulSoup   # HTML parsen (\"lesen und verstehen\")\n",
    "\n",
    "print(\"Alle Bibliotheken erfolgreich importiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was macht jede Bibliothek?\n",
    "\n",
    "| Bibliothek | Zweck | Beispiel |\n",
    "|------------|-------|----------|\n",
    "| `csv` | CSV-Dateien lesen/schreiben | `csv.writer(file)` |\n",
    "| `re` | Textmuster suchen (Regular Expressions) | `re.search(r\"BGE 151\", text)` |\n",
    "| `time` | Pausen einlegen | `time.sleep(0.5)` |\n",
    "| `requests` | Websites herunterladen | `requests.get(url)` |\n",
    "| `BeautifulSoup` | HTML analysieren | `soup.find(\"div\", id=\"regeste\")` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Teil 3: Die Funktionen im Detail\n",
    "\n",
    "## Funktion 1: `fetch_html()` - Eine Webseite herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguration: Wie wir uns gegenüber der Website \"vorstellen\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/120.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"de-CH,de;q=0.9,en;q=0.6\",\n",
    "}\n",
    "\n",
    "# Was bedeutet das?\n",
    "# - User-Agent: Sagt der Website, welchen Browser wir \"simulieren\"\n",
    "# - Accept-Language: Wir möchten deutsche Inhalte\n",
    "\n",
    "print(\"Headers konfiguriert:\")\n",
    "for key, value in HEADERS.items():\n",
    "    print(f\"  {key}: {value[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(session, url):\n",
    "    \"\"\"\n",
    "    Lädt den HTML-Inhalt einer URL herunter.\n",
    "    \n",
    "    Parameter:\n",
    "    - session: Eine requests.Session (hält Verbindungen offen)\n",
    "    - url: Die Adresse der Webseite\n",
    "    \n",
    "    Rückgabe:\n",
    "    - Der HTML-Text der Seite als String\n",
    "    \"\"\"\n",
    "    # GET-Anfrage an die URL senden\n",
    "    r = session.get(url, headers=HEADERS, timeout=30, allow_redirects=True)\n",
    "    \n",
    "    # Prüfen, ob die Anfrage erfolgreich war (Status 200 = OK)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    # Encoding sicherstellen (für Umlaute ä, ö, ü)\n",
    "    if not r.encoding:\n",
    "        r.encoding = r.apparent_encoding or \"utf-8\"\n",
    "    \n",
    "    return r.text\n",
    "\n",
    "print(\"Funktion fetch_html() definiert.\")\n",
    "print(\"\")\n",
    "print(\"Was passiert hier?\")\n",
    "print(\"1. session.get(url) → Sendet eine Anfrage an die Website\")\n",
    "print(\"2. raise_for_status() → Wirft einen Fehler, wenn etwas schiefgeht\")\n",
    "print(\"3. r.text → Gibt den HTML-Inhalt zurück\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Eine Seite herunterladen\n",
    "\n",
    "with requests.Session() as session:\n",
    "    url = \"https://search.bger.ch/ext/eurospider/live/de/php/clir/http/index.php?highlight_docid=atf%3A%2F%2F151-III-1%3Ade&lang=de&zoom=&type=show_document\"\n",
    "    html = fetch_html(session, url)\n",
    "    \n",
    "    print(f\"HTML heruntergeladen!\")\n",
    "    print(f\"Länge: {len(html)} Zeichen\")\n",
    "    print(f\"\")\n",
    "    print(f\"Die ersten 500 Zeichen:\")\n",
    "    print(html[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktion 2: `normalize_ws()` - Text aufräumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ws(s):\n",
    "    \"\"\"\n",
    "    Räumt Whitespace (Leerzeichen, Tabs, Zeilenumbrüche) auf.\n",
    "    \n",
    "    - Mehrere Leerzeichen/Tabs → ein Leerzeichen\n",
    "    - Mehr als 2 Zeilenumbrüche → maximal 2\n",
    "    - Leerzeichen am Anfang/Ende entfernen\n",
    "    \"\"\"\n",
    "    # Mehrere Leerzeichen/Tabs durch ein Leerzeichen ersetzen\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    \n",
    "    # Mehr als 2 Zeilenumbrüche durch 2 ersetzen\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    \n",
    "    # Leerzeichen am Anfang und Ende entfernen\n",
    "    return s.strip()\n",
    "\n",
    "# Beispiel:\n",
    "unordentlicher_text = \"   Viel    zu   viele     Leerzeichen   \\n\\n\\n\\n\\n und Zeilenumbrüche   \"\n",
    "sauberer_text = normalize_ws(unordentlicher_text)\n",
    "\n",
    "print(\"Vorher:\")\n",
    "print(repr(unordentlicher_text))\n",
    "print(\"\")\n",
    "print(\"Nachher:\")\n",
    "print(repr(sauberer_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exkurs: Regular Expressions (Regex)\n",
    "\n",
    "Regular Expressions sind Muster, um Text zu suchen/ersetzen:\n",
    "\n",
    "| Muster | Bedeutung |\n",
    "|--------|----------|\n",
    "| `[ \\t]+` | Ein oder mehr Leerzeichen oder Tabs |\n",
    "| `\\n{3,}` | Drei oder mehr Zeilenumbrüche |\n",
    "| `\\b` | Wortgrenze |\n",
    "| `[0-9]+` | Eine oder mehr Ziffern |\n",
    "| `.*` | Beliebig viele beliebige Zeichen |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktion 3: `extract_decision_links()` - Links zu Entscheiden finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_decision_links(index_html, base_url):\n",
    "    \"\"\"\n",
    "    Extrahiert alle Links zu Einzelentscheiden aus der Index-Seite.\n",
    "    \n",
    "    Parameter:\n",
    "    - index_html: Der HTML-Text der Index-Seite\n",
    "    - base_url: Die Basis-URL für relative Links\n",
    "    \n",
    "    Rückgabe:\n",
    "    - Liste von URLs zu den einzelnen Entscheiden\n",
    "    \"\"\"\n",
    "    # HTML mit BeautifulSoup parsen (\"verstehen\")\n",
    "    soup = BeautifulSoup(index_html, \"lxml\")\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    # Alle <a>-Tags (Links) finden, die ein href-Attribut haben\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        \n",
    "        # Relative URLs zu absoluten machen\n",
    "        abs_url = urljoin(base_url, href)\n",
    "        \n",
    "        # Nur Links zu Entscheiden behalten\n",
    "        # Diese erkennt man an \"type=show_document\" und \"highlight_docid=\"\n",
    "        if \"type=show_document\" in abs_url and \"highlight_docid=\" in abs_url:\n",
    "            links.append(abs_url)\n",
    "    \n",
    "    # Duplikate entfernen (Reihenfolge behalten)\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in links:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    \n",
    "    return out\n",
    "\n",
    "print(\"Funktion extract_decision_links() definiert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Links von der Index-Seite extrahieren\n",
    "\n",
    "INDEX_URL = \"https://search.bger.ch/ext/eurospider/live/de/php/clir/http/index_atf.php?year=151&volume=III&lang=de&zoom=&system=clir\"\n",
    "\n",
    "with requests.Session() as session:\n",
    "    index_html = fetch_html(session, INDEX_URL)\n",
    "    links = extract_decision_links(index_html, INDEX_URL)\n",
    "    \n",
    "    print(f\"Anzahl gefundener Entscheide: {len(links)}\")\n",
    "    print(\"\")\n",
    "    print(\"Die ersten 3 Links:\")\n",
    "    for i, link in enumerate(links[:3], 1):\n",
    "        print(f\"{i}. {link[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wie funktioniert BeautifulSoup?\n",
    "\n",
    "BeautifulSoup verwandelt HTML-Text in ein Objekt, das wir durchsuchen können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: BeautifulSoup verwenden\n",
    "\n",
    "beispiel_html = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"regeste\">\n",
    "      <div class=\"big bold\">Regeste</div>\n",
    "      <p>Dies ist der Inhalt.</p>\n",
    "    </div>\n",
    "    <a href=\"/seite1\">Link 1</a>\n",
    "    <a href=\"/seite2\">Link 2</a>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(beispiel_html, \"lxml\")\n",
    "\n",
    "# Element mit bestimmter ID finden\n",
    "regeste_div = soup.find(\"div\", id=\"regeste\")\n",
    "print(\"Element mit id='regeste':\")\n",
    "print(regeste_div)\n",
    "print(\"\")\n",
    "\n",
    "# Alle Links finden\n",
    "alle_links = soup.find_all(\"a\")\n",
    "print(\"Alle Links:\")\n",
    "for link in alle_links:\n",
    "    print(f\"  - {link['href']} → '{link.text}'\")\n",
    "print(\"\")\n",
    "\n",
    "# Text aus einem Element extrahieren\n",
    "text = regeste_div.get_text(\" \", strip=True)\n",
    "print(f\"Text im regeste-div: '{text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktion 4: `parse_urteilsnummer()` - Die BGE-Nummer finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_urteilsnummer(soup):\n",
    "    \"\"\"\n",
    "    Extrahiert die Urteilsnummer (z.B. \"151 III 45\") aus der Seite.\n",
    "    \n",
    "    Verwendet einen Regular Expression, um das Muster zu finden.\n",
    "    \"\"\"\n",
    "    # Gesamten Text der Seite holen\n",
    "    full_text = soup.get_text(\"\\n\", strip=True)\n",
    "    full_text = normalize_ws(full_text)\n",
    "    \n",
    "    # Muster suchen: z.B. \"151 III 45\" oder \"BGE 151 III 45\"\n",
    "    # \\b = Wortgrenze\n",
    "    # (1[0-9]{2}) = Band-Nummer (100-199)\n",
    "    # ([IVX]+) = Römische Ziffern (I, II, III, IV, V)\n",
    "    # ([0-9]{1,4}) = Seitenzahl (1-4 Ziffern)\n",
    "    m = re.search(r\"\\b(?:BGE\\s*)?(1[0-9]{2})\\s+([IVX]+)\\s+([0-9]{1,4})\\b\", full_text)\n",
    "    \n",
    "    if not m:\n",
    "        return \"\"\n",
    "    \n",
    "    # Gefundene Gruppen zusammensetzen\n",
    "    return f\"{m.group(1)} {m.group(2)} {m.group(3)}\"\n",
    "\n",
    "print(\"Funktion parse_urteilsnummer() definiert.\")\n",
    "print(\"\")\n",
    "print(\"Der Regex erklärt:\")\n",
    "print(\"  \\\\b(?:BGE\\\\s*)?(1[0-9]{2})\\\\s+([IVX]+)\\\\s+([0-9]{1,4})\\\\b\")\n",
    "print(\"  │    │          │            │           │\")\n",
    "print(\"  │    │          │            │           └── Seitenzahl (z.B. 45)\")\n",
    "print(\"  │    │          │            └── Röm. Ziffern (z.B. III)\")\n",
    "print(\"  │    │          └── Band (z.B. 151)\")\n",
    "print(\"  │    └── Optionales 'BGE '\")\n",
    "print(\"  └── Wortgrenze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Urteilsnummer extrahieren\n",
    "\n",
    "test_texte = [\n",
    "    \"BGE 151 III 45 ist ein wichtiger Entscheid\",\n",
    "    \"Siehe auch 151 III 1 und 151 III 9\",\n",
    "    \"Hier steht keine Nummer\",\n",
    "]\n",
    "\n",
    "for text in test_texte:\n",
    "    m = re.search(r\"\\b(?:BGE\\s*)?(1[0-9]{2})\\s+([IVX]+)\\s+([0-9]{1,4})\\b\", text)\n",
    "    if m:\n",
    "        nummer = f\"{m.group(1)} {m.group(2)} {m.group(3)}\"\n",
    "        print(f\"'{text}' → Gefunden: {nummer}\")\n",
    "    else:\n",
    "        print(f\"'{text}' → Nichts gefunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktion 5: `parse_regeste()` - Die Regeste extrahieren\n",
    "\n",
    "**Dies ist die wichtigste Funktion!** Hier nutzen wir unser Wissen über die HTML-Struktur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_regeste(soup):\n",
    "    \"\"\"\n",
    "    Extrahiert alle Regesten aus der Seite.\n",
    "    \n",
    "    Die HTML-Struktur ist:\n",
    "    <div id=\"regeste\" lang=\"de\">\n",
    "      <div class=\"big bold\">Regeste</div>  (oder \"Regeste a\", \"Regeste b\" etc.)\n",
    "      <div class=\"paraatf\">\n",
    "        <span class=\"artref\">Art. XY</span>; Titel...\n",
    "        <div class=\"paratf\">Eigentlicher Text...</div>\n",
    "      </div>\n",
    "    </div>\n",
    "    \n",
    "    Bei mehreren Regesten wird \"Regeste a:\", \"Regeste b:\" etc. vorangestellt.\n",
    "    Bei nur einer Regeste wird kein Label verwendet.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SCHRITT 1: Alle <div id=\"regeste\"> finden\n",
    "    regeste_divs = soup.find_all(\"div\", id=\"regeste\")\n",
    "    \n",
    "    if not regeste_divs:\n",
    "        return \"\"  # Keine Regeste gefunden\n",
    "    \n",
    "    regesten = []\n",
    "    \n",
    "    # SCHRITT 2: Jede Regeste verarbeiten\n",
    "    for div in regeste_divs:\n",
    "        # Label extrahieren (z.B. \"Regeste\" oder \"Regeste a\")\n",
    "        label_div = div.find(\"div\", class_=\"big bold\")\n",
    "        label = label_div.get_text(strip=True) if label_div else \"\"\n",
    "        \n",
    "        # Inhalt extrahieren aus <div class=\"paraatf\">\n",
    "        content_div = div.find(\"div\", class_=\"paraatf\")\n",
    "        if not content_div:\n",
    "            continue\n",
    "        \n",
    "        # Text sauber extrahieren\n",
    "        content = content_div.get_text(\" \", strip=True)\n",
    "        content = normalize_ws(content)\n",
    "        \n",
    "        if content:\n",
    "            regesten.append((label, content))\n",
    "    \n",
    "    if not regesten:\n",
    "        return \"\"\n",
    "    \n",
    "    # SCHRITT 3: Formatierung\n",
    "    # Bei nur einer Regeste: kein Label\n",
    "    if len(regesten) == 1:\n",
    "        return regesten[0][1]  # Nur den Inhalt zurückgeben\n",
    "    \n",
    "    # Bei mehreren Regesten: mit Label\n",
    "    parts = []\n",
    "    for label, content in regesten:\n",
    "        # Label normalisieren (geschütztes Leerzeichen entfernen)\n",
    "        label_clean = label.replace(\"\\u00a0\", \" \").strip()\n",
    "        parts.append(f\"{label_clean}:\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "print(\"Funktion parse_regeste() definiert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Regeste aus echtem HTML extrahieren\n",
    "\n",
    "test_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "  <div id=\"regeste\" lang=\"de\">\n",
    "    <div class=\"big bold\">Regeste a</div>\n",
    "    <br>\n",
    "    <div class=\"paraatf\">\n",
    "      <span class=\"artref\">Art. 51 Abs. 4 BGG</span>; Streitwert bei Rechtsöffnung.\n",
    "      <div class=\"paratf\">Künftige Unterhaltsbeiträge sind noch nicht fällig.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <div id=\"regeste\" lang=\"de\">\n",
    "    <div class=\"big bold\">Regeste b</div>\n",
    "    <br>\n",
    "    <div class=\"paraatf\">\n",
    "      <span class=\"artref\">Art. 84 SchKG</span>; Kognition des Rechtsöffnungsgerichts.\n",
    "      <div class=\"paratf\">Die Rechtsöffnung ist zu verweigern.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(test_html, \"lxml\")\n",
    "regeste = parse_regeste(soup)\n",
    "\n",
    "print(\"Extrahierte Regeste:\")\n",
    "print(\"=\" * 50)\n",
    "print(regeste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Teil 4: Die Hauptfunktion `main()`\n",
    "\n",
    "Hier wird alles zusammengeführt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Hauptfunktion: Führt den gesamten Scraping-Prozess durch.\n",
    "    \"\"\"\n",
    "    INDEX_URL = \"https://search.bger.ch/ext/eurospider/live/de/php/clir/http/index_atf.php?year=151&volume=III&lang=de&zoom=&system=clir\"\n",
    "    out_csv = \"bge_151_iii_regesten.csv\"\n",
    "    \n",
    "    # Session erstellen (hält Verbindungen offen)\n",
    "    with requests.Session() as session:\n",
    "        \n",
    "        # SCHRITT 1: Index-Seite laden\n",
    "        print(\"Lade Index-Seite...\")\n",
    "        index_html = fetch_html(session, INDEX_URL)\n",
    "        \n",
    "        # SCHRITT 2: Links extrahieren\n",
    "        decision_links = extract_decision_links(index_html, INDEX_URL)\n",
    "        print(f\"Gefunden: {len(decision_links)} Entscheide\")\n",
    "        \n",
    "        if not decision_links:\n",
    "            raise RuntimeError(\"Keine Entscheid-Links gefunden!\")\n",
    "        \n",
    "        # SCHRITT 3: Jeden Entscheid besuchen\n",
    "        rows = []\n",
    "        for i, url in enumerate(decision_links, start=1):\n",
    "            # Höflich sein: 0.5 Sekunden zwischen Anfragen warten\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Seite laden und parsen\n",
    "            html = fetch_html(session, url)\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            \n",
    "            # Daten extrahieren\n",
    "            urteilsnummer = parse_urteilsnummer(soup)\n",
    "            regeste = parse_regeste(soup)\n",
    "            \n",
    "            # Warnungen ausgeben\n",
    "            if not urteilsnummer:\n",
    "                print(f\"[WARN] Urteilsnummer nicht gefunden: {url}\")\n",
    "            if not regeste:\n",
    "                print(f\"[WARN] Regeste nicht gefunden: {url}\")\n",
    "            \n",
    "            # Ergebnis speichern\n",
    "            rows.append((urteilsnummer, regeste))\n",
    "            print(f\"[{i}/{len(decision_links)}] {urteilsnummer} extrahiert\")\n",
    "    \n",
    "    # SCHRITT 4: CSV schreiben\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=\";\")  # Semikolon für CH-Excel\n",
    "        writer.writerow([\"urteilsnummer\", \"regeste\"])  # Header\n",
    "        writer.writerows(rows)  # Daten\n",
    "    \n",
    "    print(f\"Fertig: {out_csv}\")\n",
    "\n",
    "print(\"Funktion main() definiert.\")\n",
    "print(\"\")\n",
    "print(\"Der Ablauf:\")\n",
    "print(\"1. Index-Seite laden\")\n",
    "print(\"2. Links zu allen Entscheiden extrahieren\")\n",
    "print(\"3. Jeden Entscheid besuchen und Daten extrahieren\")\n",
    "print(\"4. Alles in eine CSV-Datei schreiben\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warum `time.sleep(0.5)`?\n",
    "\n",
    "**Höflichkeit!** Wenn wir zu schnell viele Anfragen senden, könnte:\n",
    "- Die Website uns blockieren\n",
    "- Der Server überlastet werden\n",
    "\n",
    "Mit `time.sleep(0.5)` warten wir eine halbe Sekunde zwischen den Anfragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Teil 5: Das alte Problem und die Lösung\n",
    "\n",
    "## Was war das Problem mit dem ursprünglichen Code?\n",
    "\n",
    "Der ursprüngliche Code suchte nach einem Element mit **exakt** dem Text `\"Regeste\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTER CODE (funktionierte nicht bei allen Entscheiden):\n",
    "\n",
    "def alte_methode(soup, label):\n",
    "    \"\"\"Sucht nach einem Element mit exakt diesem Text.\"\"\"\n",
    "    node = soup.find(string=lambda t: isinstance(t, str) and t.strip() == label)\n",
    "    return node\n",
    "\n",
    "# Das Problem: Bei manchen Entscheiden steht \"Regeste a\" statt \"Regeste\"\n",
    "\n",
    "test_html = '<div class=\"big bold\">Regeste a</div>'\n",
    "soup = BeautifulSoup(test_html, \"lxml\")\n",
    "\n",
    "gefunden = alte_methode(soup, \"Regeste\")\n",
    "print(f\"Suche nach 'Regeste': {gefunden}\")\n",
    "\n",
    "gefunden = alte_methode(soup, \"Regeste a\")\n",
    "print(f\"Suche nach 'Regeste a': {gefunden}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Lösung\n",
    "\n",
    "Statt nach dem Text zu suchen, suchen wir nach der **HTML-Struktur**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEUE METHODE (funktioniert immer):\n",
    "\n",
    "def neue_methode(soup):\n",
    "    \"\"\"Sucht nach allen <div id='regeste'> Elementen.\"\"\"\n",
    "    return soup.find_all(\"div\", id=\"regeste\")\n",
    "\n",
    "test_html = \"\"\"\n",
    "<div id=\"regeste\"><div class=\"big bold\">Regeste a</div><div class=\"paraatf\">Inhalt A</div></div>\n",
    "<div id=\"regeste\"><div class=\"big bold\">Regeste b</div><div class=\"paraatf\">Inhalt B</div></div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(test_html, \"lxml\")\n",
    "regesten = neue_methode(soup)\n",
    "\n",
    "print(f\"Gefunden: {len(regesten)} Regeste(n)\")\n",
    "for i, r in enumerate(regesten, 1):\n",
    "    label = r.find(\"div\", class_=\"big bold\").get_text(strip=True)\n",
    "    print(f\"  {i}. {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Teil 6: Zusammenfassung\n",
    "\n",
    "## Die wichtigsten Konzepte:\n",
    "\n",
    "### 1. HTTP-Anfragen mit `requests`\n",
    "```python\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "```\n",
    "\n",
    "### 2. HTML parsen mit `BeautifulSoup`\n",
    "```python\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "element = soup.find(\"div\", id=\"regeste\")\n",
    "alle_elemente = soup.find_all(\"a\", href=True)\n",
    "text = element.get_text(strip=True)\n",
    "```\n",
    "\n",
    "### 3. Text suchen mit Regular Expressions\n",
    "```python\n",
    "import re\n",
    "match = re.search(r\"Muster\", text)\n",
    "neuer_text = re.sub(r\"alt\", \"neu\", text)\n",
    "```\n",
    "\n",
    "### 4. CSV schreiben\n",
    "```python\n",
    "with open(\"datei.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\";\")\n",
    "    writer.writerow([\"spalte1\", \"spalte2\"])\n",
    "    writer.writerows(daten)\n",
    "```\n",
    "\n",
    "## Der Schlüssel zum Erfolg:\n",
    "\n",
    "1. **HTML-Struktur analysieren** - Entwicklertools nutzen (F12)\n",
    "2. **Muster erkennen** - Was haben alle Regesten gemeinsam?\n",
    "3. **Gezielt suchen** - Nach Struktur, nicht nach Text\n",
    "4. **Testen** - Mit verschiedenen Seiten prüfen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus: Den Scraper selbst ausführen\n",
    "\n",
    "Du kannst den Scraper direkt hier ausführen (dauert ca. 30 Sekunden):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entkommentiere die nächste Zeile, um den Scraper auszuführen:\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Gratulation!** Du hast jetzt einen Überblick, wie Web Scraping funktioniert.\n",
    "\n",
    "Bei Fragen: Experimentiere mit dem Code, ändere Dinge, und schau was passiert!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
